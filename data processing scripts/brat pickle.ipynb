{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Parse all the text and ann files from Brat output and store the results in a pickle file to speed-up dataset creation\n",
    "########## For details of functions, refer to Bert Dataset v1 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, requests, json, collections, os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "from numpy.random import randint\n",
    "regex = re.compile('[^a-zA-Z0-9-_]')\n",
    "import xlwt \n",
    "from xlwt import Workbook\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import pos_tag\n",
    "import stanza\n",
    "import pickle\n",
    "nlp = stanza.Pipeline('en',verbose=False, processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_properties = {}\n",
    "category_properties['Hardware-Devices'] = ['notRelevant','isPowered','isConnected','isSetup','isUsed']\n",
    "category_properties['Software-Device-Drivers'] = ['notRelevant','isInstalled','isRelatedDeviceConnected','isSettingsChanged']\n",
    "category_properties['Software-OS-Related'] = ['notRelevant','isOpened','isSettingsChanged']\n",
    "category_properties['Software-Other'] = ['notRelevant','isInstalled','isOpened','isSettingsChanged']\n",
    "category_properties['Hardware-Other'] = ['']\n",
    "\n",
    "property_event = {}\n",
    "property_event['notRelevant'] = True\n",
    "property_event['isPowered'] = False\n",
    "property_event['isConnected'] = False\n",
    "property_event['isSetup'] = True\n",
    "property_event['isUsed'] = True\n",
    "property_event['isInstalled'] = False\n",
    "property_event['isRelatedDeviceConnected'] = False\n",
    "property_event['isSettingsChanged'] = True\n",
    "property_event['isOpened'] = False\n",
    "\n",
    "property_values = {}\n",
    "property_values[True] = ['true','false']\n",
    "property_values[False] = ['f->t','t->f','noChange','start-T','start-F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_details = []\n",
    "mentions = {}\n",
    "entities = {}\n",
    "ent_variants = {}\n",
    "ent_verb_spans = {}\n",
    "merged_ents = {}\n",
    "step_wise_properties = {}\n",
    "\n",
    "all_annotations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(offset):\n",
    "    for i, step in enumerate(step_details):\n",
    "        if(int(offset)<step[2]):\n",
    "            return i-2\n",
    "    return len(step_details)-1\n",
    "def clean_text(ent_text):\n",
    "    tokens=nlp(ent_text)\n",
    "    tokens = [w for sent in tokens.sentences for w in sent.words]\n",
    "    lemmas = [token.lemma.lower().strip() for token in tokens]\n",
    "    ent_text = ' '.join(lemmas)\n",
    "    return ent_text\n",
    "def get_text(mention):\n",
    "    text = mentions[mention][-2]\n",
    "    while(entities[text][0] != '' and entities[text][0] != text):\n",
    "        text = entities[text][0]\n",
    "    return text\n",
    "def get_base_ent(text):\n",
    "    while(entities[text][0] != '' and entities[text][0] != text):\n",
    "        text = entities[text][0]\n",
    "    return text\n",
    "def state_change(new_state, prev_state):\n",
    "    if(new_state==prev_state):\n",
    "        return 'None'\n",
    "    elif(new_state=='True'):\n",
    "        return 'True'\n",
    "    elif(new_state=='False'):\n",
    "        return 'False'\n",
    "    else:\n",
    "        return 'None'\n",
    "def get_indexes(sent_tokens, ent_text,verb=False):\n",
    "    indexes = ['0' for _ in sent_tokens]\n",
    "    if verb:\n",
    "        indexes = ['1' if token.pos=='VERB' else '0' for token in sent_tokens]\n",
    "    else:\n",
    "        for ent in ent_variants[ent_text]:\n",
    "            ent_tokens = nlp(ent)\n",
    "            ent_tokens = [w for sent in ent_tokens.sentences for w in sent.words]\n",
    "            for i in range(len(sent_tokens)-len(ent_tokens)+1):\n",
    "                if(all(ent_tokens[j].lemma.lower().strip()==sent_tokens[i+j].lemma.lower().strip() for j in range(len(ent_tokens)))):\n",
    "                    for j in range(len(ent_tokens)):\n",
    "                        indexes[i+j]='1'\n",
    "    return indexes\n",
    "def process_sent(sent:str,ent_text:str):\n",
    "    tokens=nlp(sent)\n",
    "    tokens = [w for sent in tokens.sentences for w in sent.words]\n",
    "    lemmas = [token.lemma.lower().strip() for token in tokens]\n",
    "    ent_spans = get_indexes(tokens,ent_text)\n",
    "    verb_spans = get_indexes(tokens, ent_text, verb=True)\n",
    "    return [lemmas,ent_spans,verb_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(out_file,doc_id=1,folder='config/',file='monitor',filter_prop=''):\n",
    "    global entities, mentions, step_details, ent_variants, all_annotations\n",
    "    try:\n",
    "        config_file = folder+file+'.ann'\n",
    "        text_file = folder+file+'.txt'\n",
    "        annotations = open(config_file,'r',encoding='utf-8').readlines()\n",
    "        raw_text = open(text_file,'r',encoding='utf-8').read().strip()\n",
    "        steps = raw_text.split('\\n\\n')\n",
    "\n",
    "        step_details = []\n",
    "        start = 0\n",
    "        start2 = 0\n",
    "        for i, step in enumerate(steps):\n",
    "            length = len(step)\n",
    "            newlines = step.count('\\n')\n",
    "            step_details.append([i,step,start,start2,length,newlines])\n",
    "            start+=length+newlines+4 ### accounting for '\\n' diff in Brat\n",
    "            start2+=2+length\n",
    "        #     if(i==3):\n",
    "        #         print(step,'\\n\\n',newlines)\n",
    "        num_steps = len(step_details) - 1\n",
    "\n",
    "        entities = {} #base-entity, mentions\n",
    "        mentions = {} #ent_id, line_no, ent_cat, start, end, ent_text, properties\n",
    "        for line in annotations:\n",
    "            line=line.strip()\n",
    "            if(line[0]=='T'): ### entities\n",
    "                ent_id, ent_cat, ent_text = line.split('\\t')\n",
    "                ent_text = clean_text(ent_text)\n",
    "                ent_cat, start, end = ent_cat.split(' ')\n",
    "                line_no = get_line(start)\n",
    "                start,end,line_no = int(start),int(end),int(line_no)\n",
    "                mentions[ent_id] = [ent_id, line_no,ent_cat,start,end,ent_text,{}]\n",
    "                if(ent_text not in entities):\n",
    "                    entities[ent_text]=['',[]]\n",
    "                entities[ent_text][1].append(ent_id)\n",
    "        #         print(ent_id,ent_cat,start,end,line_no,ent_text,sep='\\n')\n",
    "        #         break\n",
    "            elif(line[0]=='A'): ### attributes\n",
    "                atr_id, atr_type = line.split('\\t')\n",
    "                split = atr_type.split(' ')\n",
    "                if(len(split)==2):\n",
    "                    split.append('true')\n",
    "                atr_type, atr_ent, atr_val = split\n",
    "                mentions[atr_ent][-1][atr_type]=atr_val\n",
    "        #         print(atr_id,atr_type, atr_ent, atr_val,sep='\\n')\n",
    "        #         break\n",
    "            elif(line[0]=='*'): ### equiv\n",
    "                _, rel_type = line.split('\\t')\n",
    "                rels = rel_type.split(' ')\n",
    "                rel_type, ent1, ent2 = rels[0], rels[1], rels[2]\n",
    "                text1, text2 = get_text(ent1), get_text(ent2)\n",
    "                if(text1==text2):\n",
    "                    continue\n",
    "                base, other = text2, text1\n",
    "                if(len(text1)<=len(text2)):\n",
    "                    base, other = text1, text2\n",
    "                entities[other][0]=base\n",
    "        #         print(entities[other][0],base,sep=',')\n",
    "            elif(line[0]=='R'): ### relation- subpart\n",
    "                rel_id, rel_type = line.split('\\t')\n",
    "                rel_type, sub, main = rel_type.split(' ')\n",
    "                sub, main = sub.split(':')[1],main.split(':')[1]\n",
    "                sub, main = get_text(sub), get_text(main)\n",
    "                entities[sub][0] = main\n",
    "        #         print(rel_id,rel_type,sub,main,sep=',\\t')\n",
    "            else:\n",
    "                print(line)\n",
    "\n",
    "        merged_ents = {} ### merge entities with base-subpart-equiv\n",
    "        ent_variants = {}\n",
    "        for ent in entities:\n",
    "            base_ent = get_base_ent(ent)\n",
    "            if(base_ent not in merged_ents):\n",
    "                merged_ents[base_ent] = {}\n",
    "                ent_variants[base_ent] = set()\n",
    "            ent_variants[base_ent].add(ent)\n",
    "            for mention_id in entities[ent][1]:\n",
    "                mention = mentions[mention_id]\n",
    "                line_no = mention[1]\n",
    "                if(line_no not in merged_ents[base_ent]):\n",
    "                    merged_ents[base_ent][line_no] = mention\n",
    "                else:\n",
    "                    merged_ents[base_ent][line_no][-1].update(mention[-1])\n",
    "    #     print(ent_variants)\n",
    "\n",
    "        clean_ents = []\n",
    "        for ent in merged_ents:\n",
    "            if(all(merged_ents[ent][line_no][-1]=={} for line_no in merged_ents[ent])):\n",
    "                clean_ents.append(ent)\n",
    "        for ent in clean_ents:\n",
    "            del merged_ents[ent]\n",
    "\n",
    "        step_wise_properties = {}\n",
    "\n",
    "        for ent in merged_ents:\n",
    "            mention = mentions[entities[ent][1][0]]\n",
    "            category = mention[2]\n",
    "            if(category == 'Hardware-Other'):\n",
    "                continue\n",
    "            properties = category_properties[category]\n",
    "            ent_properties = {}\n",
    "            for prop in properties:\n",
    "                prop_val = ['']*(num_steps+1)\n",
    "                prop_val[0]='False'\n",
    "        #         print(sorted(merged_ents[ent]))\n",
    "        #         break\n",
    "        #         ent_mentions = [mentio]\n",
    "        #         ent_mentions = sorted(merged_ents[ent])\n",
    "                if(property_event[prop]):\n",
    "                    for step_no in range(0,num_steps):\n",
    "                        if(step_no in merged_ents[ent] and prop in merged_ents[ent][step_no][-1]):\n",
    "                            val = merged_ents[ent][step_no][-1][prop].capitalize()\n",
    "                            prop_val[step_no+1] = val\n",
    "                        else:\n",
    "                            prop_val[step_no+1] = 'False'\n",
    "                else:\n",
    "                    for step_no in range(0,num_steps):\n",
    "                        if(step_no in merged_ents[ent] and prop in merged_ents[ent][step_no][-1]):\n",
    "                            val = merged_ents[ent][step_no][-1][prop]\n",
    "                            if(val=='f->t' and prop_val[step_no]=='False'):\n",
    "                                prop_val[step_no+1] = 'True'\n",
    "                            elif(val=='t->f' and prop_val[step_no]=='True'):\n",
    "                                prop_val[step_no+1] = 'False'\n",
    "                            elif(val=='t->f' and all(val=='False' for val in prop_val[:step_no])):\n",
    "                                for i in range(step_no+1):\n",
    "                                    prop_val[i] = 'True'\n",
    "                                prop_val[step_no+1] = 'False'\n",
    "                            elif(val=='start-T'):\n",
    "                                for i in range(step_no+2):\n",
    "                                    prop_val[step_no+1] = 'True'\n",
    "                            elif(val=='start-F'):\n",
    "                                for i in range(step_no+2):\n",
    "                                    prop_val[step_no+1] = 'False'\n",
    "                            else:\n",
    "                                prop_val[step_no+1] = prop_val[step_no]\n",
    "                        else:\n",
    "                            prop_val[step_no+1] = prop_val[step_no]\n",
    "                if not((filter_prop != '' and prop!=filter_prop) or all(val=='False' for val in prop_val)):\n",
    "                    ent_properties[prop]=prop_val\n",
    "        #     print(ent,category,ent_properties,sep='\\n')\n",
    "        #     break\n",
    "            step_wise_properties[ent]=ent_properties\n",
    "        # print(step_wise_properties)\n",
    "\n",
    "#         df = pd.DataFrame({'Steps':steps})\n",
    "#         for ent in step_wise_properties:\n",
    "#             for prop in step_wise_properties[ent]:\n",
    "#                 if(filter_prop==''):\n",
    "#                     df[ent+'___'+prop] = step_wise_properties[ent][prop]\n",
    "#                 else:\n",
    "#                     df[ent] = step_wise_properties[ent][prop]\n",
    "\n",
    "#         headers = list(df.columns)[1:]\n",
    "    #     print(df['Steps'])\n",
    "        ent_verb_spans = {}\n",
    "        for ent in step_wise_properties:\n",
    "            ent_verb_spans[ent]={}\n",
    "            for prop in step_wise_properties[ent]:\n",
    "                for step_no in range(len(steps)-1):\n",
    "                    sent_lemmas, ent_spans, verb_spans = process_sent(steps[step_no+1],ent)\n",
    "                    if(step_no not in ent_verb_spans[ent]):\n",
    "                        ent_verb_spans[ent][step_no] = (sent_lemmas, ent_spans, verb_spans)\n",
    "#                     if(all(ent_tag=='0' for ent_tag in ent_spans)):\n",
    "#                         continue;\n",
    "#     #                 print(sent_lemmas, ent_spans, verb_spans)\n",
    "#                     print('####'.join(sent_lemmas),','.join(verb_spans),','.join(ent_spans),state_change(step_wise_properties[ent][prop][step_no+1],step_wise_properties[ent][prop][step_no]),sep='\\t',file=out_file)\n",
    "        all_annotations[file] = {}\n",
    "        all_annotations[file]['steps'] = steps\n",
    "        all_annotations[file]['step_details'] = step_details\n",
    "        all_annotations[file]['mentions'] = mentions\n",
    "        all_annotations[file]['entities'] = entities\n",
    "        all_annotations[file]['ent_variants'] = ent_variants\n",
    "        all_annotations[file]['ent_verb_spans'] = ent_verb_spans\n",
    "        all_annotations[file]['merged_ents'] = merged_ents\n",
    "        all_annotations[file]['step_wise_properties'] = step_wise_properties\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_annotations()\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prop(filter_prop='isOpened'):\n",
    "#     filter_prop = 'isOpened'\n",
    "    train_f = open('./annotations/prolocal_annotations_'+filter_prop+'_train.tsv','w',encoding='utf-8')\n",
    "    dev_f = open('./annotations/prolocal_annotations_'+filter_prop+'_dev.tsv','w',encoding='utf-8')\n",
    "    test_f = open('./annotations/prolocal_annotations_'+filter_prop+'_test.tsv','w',encoding='utf-8')\n",
    "    data_folder = './annotations/'\n",
    "    count = 1\n",
    "    for folder in [fol for fol in listdir(data_folder) if not isfile(join(data_folder, fol))]:\n",
    "        folder = data_folder + folder+'/'\n",
    "        onlyfiles = [f[:-4] for f in listdir(folder) if f.endswith('.txt')]\n",
    "        for file in onlyfiles:\n",
    "            rand = randint(0,100)\n",
    "            print(count, folder, file,rand)\n",
    "            if(rand<80):\n",
    "                parse_annotations(out_file=train_f,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            elif(rand<90):\n",
    "                parse_annotations(out_file=dev_f,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            else:\n",
    "                parse_annotations(out_file=test_f,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            count+=1\n",
    "    train_f.close()\n",
    "    dev_f.close()\n",
    "    test_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parse_prop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_annotations,open('./annotations/all_files_v1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_annotations(open('temp.txt','w',encoding='utf-8'),169,'./annotations/Ubuntu/','654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annotations['199_1___Test_Your_PS4_Controller___Steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:entities] *",
   "language": "python",
   "name": "conda-env-entities-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
