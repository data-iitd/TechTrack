{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Parse results from Brat Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, requests, json, collections, os, pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "regex = re.compile('[^a-zA-Z0-9-_]')\n",
    "import xlwt \n",
    "from xlwt import Workbook\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import pos_tag\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('en',verbose=False, processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Configuring the properties and entity categories\n",
    "category_properties = {}\n",
    "category_properties['Hardware-Devices'] = ['notRelevant','isPowered','isConnected','isSetup','isUsed']\n",
    "category_properties['Software-Device-Drivers'] = ['notRelevant','isInstalled','isRelatedDeviceConnected','isSettingsChanged']\n",
    "category_properties['Software-OS-Related'] = ['notRelevant','isOpened','isSettingsChanged']\n",
    "category_properties['Software-Other'] = ['notRelevant','isInstalled','isOpened','isSettingsChanged']\n",
    "category_properties['Hardware-Other'] = ['']\n",
    "\n",
    "property_event = {}\n",
    "property_event['notRelevant'] = True\n",
    "property_event['isPowered'] = False\n",
    "property_event['isConnected'] = False\n",
    "property_event['isSetup'] = True\n",
    "property_event['isUsed'] = True\n",
    "property_event['isInstalled'] = False\n",
    "property_event['isRelatedDeviceConnected'] = False\n",
    "property_event['isSettingsChanged'] = True\n",
    "property_event['isOpened'] = False\n",
    "\n",
    "property_values = {}\n",
    "property_values[True] = ['true','false']\n",
    "property_values[False] = ['f->t','t->f','noChange','start-T','start-F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_details = []\n",
    "mentions = {}\n",
    "entities = {}\n",
    "ent_variants = {}\n",
    "ent_verb_spans = {}\n",
    "merged_ents = {}\n",
    "step_wise_properties = {}\n",
    "ent_wise_spans = {}\n",
    "\n",
    "all_annotations = pickle.load(open('./annotations/all_files_v1_complete.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ A wide variety of util functions\n",
    "\n",
    "# def process_word(word):\n",
    "#     w = word\n",
    "#     cap = False\n",
    "#     if(w.isupper()):\n",
    "#         cap = True\n",
    "#     w = w.lower()\n",
    "# #     print(w)\n",
    "#     if('\\'s' == w):\n",
    "#         w=''\n",
    "#     if('cd' == w):\n",
    "#         w='CD'\n",
    "#     w = lemmatizer.lemmatize(w)\n",
    "# #     print(w)\n",
    "#     if(not cap):\n",
    "#         w = w.capitalize()\n",
    "#     else:\n",
    "#         w = w.upper()\n",
    "#     w = regex.sub('', w)\n",
    "#     if(len(w)<2 and w!= 'X'):\n",
    "#         w=''\n",
    "#     return w\n",
    "##### Get line number for an entity\n",
    "def get_line(offset):\n",
    "    for i, step in enumerate(step_details):\n",
    "        if(int(offset)<step[2]):\n",
    "            return i-2\n",
    "    return len(step_details)-1\n",
    "\n",
    "##### Clean up the text by lemmatizing\n",
    "def clean_text(ent_text):\n",
    "    tokens=nlp(ent_text)\n",
    "    tokens = [w for sent in tokens.sentences for w in sent.words]\n",
    "    lemmas = [token.lemma.lower().strip() for token in tokens]\n",
    "    ent_text = ' '.join(lemmas)\n",
    "    return ent_text\n",
    "\n",
    "##### Get base text for a mention ID\n",
    "def get_text(mention):\n",
    "    text = mentions[mention][-2]\n",
    "    while(entities[text][0] != '' and entities[text][0] != text):\n",
    "        text = entities[text][0]\n",
    "    return text\n",
    "##### get base text for a given entity form: example printers -> printer\n",
    "def get_base_ent(text):\n",
    "    while(entities[text][0] != '' and entities[text][0] != text):\n",
    "        text = entities[text][0]\n",
    "    return text\n",
    "def state_change(new_state, prev_state):\n",
    "    if(new_state==prev_state):\n",
    "        return 'None'\n",
    "    elif(new_state=='True'):\n",
    "        return 'True'\n",
    "    elif(new_state=='False'):\n",
    "        return 'False'\n",
    "    else:\n",
    "        return 'None'\n",
    "####### Get indexes for noun and verb positions\n",
    "def get_indexes(sent_tokens, ent_text,verb=False):\n",
    "    indexes = ['0' for _ in sent_tokens]\n",
    "    if verb:\n",
    "        indexes = ['1' if token.pos=='VERB' else '0' for token in sent_tokens]\n",
    "    else:\n",
    "        for ent in ent_variants[ent_text]:\n",
    "            ent_tokens = nlp(ent)\n",
    "            ent_tokens = [w for sent in ent_tokens.sentences for w in sent.words]\n",
    "            for i in range(len(sent_tokens)-len(ent_tokens)+1):\n",
    "                if(all(ent_tokens[j].lemma.lower().strip()==sent_tokens[i+j].lemma.lower().strip() for j in range(len(ent_tokens)))):\n",
    "                    for j in range(len(ent_tokens)):\n",
    "                        indexes[i+j]='1'\n",
    "    return indexes\n",
    "def process_sent(sent:str,ent_text:str):\n",
    "    tokens=nlp(sent)\n",
    "    tokens = [w for sent in tokens.sentences for w in sent.words]\n",
    "    lemmas = [token.lemma.lower().strip() for token in tokens]\n",
    "    ent_spans = get_indexes(tokens,ent_text)\n",
    "    verb_spans = get_indexes(tokens, ent_text, verb=True)\n",
    "    return [lemmas,ent_spans,verb_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Parse annotations from a file and store it in state-change-type dataset format\n",
    "######## Ignore the file in case of any errors whatsoever\n",
    "def parse_annotations(out_file,doc_id=1,folder='config/',file='monitor',filter_prop='',model='bert',test_files={}):\n",
    "    global step_details, mentions,entities,ent_variants,ent_verb_spans,merged_ents,step_wise_properties,ent_wise_spans\n",
    "    try:\n",
    "        steps = all_annotations[file]['steps']\n",
    "        step_details = all_annotations[file]['step_details']\n",
    "        mentions = all_annotations[file]['mentions']\n",
    "        entities = all_annotations[file]['entities']\n",
    "        ent_variants = all_annotations[file]['ent_variants']\n",
    "        ent_verb_spans = all_annotations[file]['ent_verb_spans']\n",
    "        merged_ents = all_annotations[file]['merged_ents']\n",
    "        step_wise_properties = all_annotations[file]['step_wise_properties']\n",
    "        for ent in step_wise_properties:\n",
    "            for prop in step_wise_properties[ent]:\n",
    "                if(filter_prop != '' and prop!=filter_prop):\n",
    "                    continue\n",
    "                for step_no in range(len(steps)-1):\n",
    "                    if(model=='bert'):\n",
    "                        query=''\n",
    "                        ######## Convert query to natural text\n",
    "                        if(prop=='isOpened'):\n",
    "                            query = 'Is '+ ent +' opened?'\n",
    "                        elif(prop=='isSettingsChanged'):\n",
    "                            query = 'Were settings of ' + ent + ' changed?'\n",
    "                        elif(prop=='isInstalled'):\n",
    "                            query = 'Is ' + ent + ' installed?'\n",
    "                        elif(prop=='isRelatedDeviceConnected'):\n",
    "                            query = 'Is the device for ' + ent + ' connected?'\n",
    "                        elif(prop=='isConnected'):\n",
    "                            query = 'Is ' + ent + ' connected?'\n",
    "                        elif(prop=='isSetup'):\n",
    "                            query = 'Was ' + ent + ' setup?'\n",
    "                        elif(prop=='isPowered'):\n",
    "                            query = 'Is ' + ent + ' powered on?'\n",
    "                        elif(prop=='isUsed'):\n",
    "                            query = 'Was ' + ent + ' used?'\n",
    "                        elif(prop=='notRelevant'):\n",
    "                            continue\n",
    "                        else:\n",
    "                            print('ERROR '+prop)\n",
    "                        if(property_event[prop]):\n",
    "                            print(repr(query),repr(steps[step_no+1]),step_wise_properties[ent][prop][step_no+1],sep='\\t\\t\\t',file=out_file)\n",
    "                        else:\n",
    "                            print(repr(query),repr(steps[step_no+1]),state_change(step_wise_properties[ent][prop][step_no+1],step_wise_properties[ent][prop][step_no]),sep='\\t\\t\\t',file=out_file)\n",
    "                        if(prop in test_files):\n",
    "                            if(property_event[prop]):\n",
    "                                print(repr(query),repr(steps[step_no+1]),step_wise_properties[ent][prop][step_no+1],sep='\\t\\t\\t',file=test_files[prop])\n",
    "                            else:\n",
    "                                print(repr(query),repr(steps[step_no+1]),state_change(step_wise_properties[ent][prop][step_no+1],step_wise_properties[ent][prop][step_no]),sep='\\t\\t\\t',file=test_files[prop])\n",
    "                    elif(model=='prolocal'):\n",
    "                        sent_lemmas, ent_spans, verb_spans = ent_verb_spans[ent][step_no]\n",
    "                        if(property_event[prop]):\n",
    "                            print('####'.join(sent_lemmas),','.join(verb_spans),','.join(ent_spans),step_wise_properties[ent][prop][step_no+1],sep='\\t',file=out_file)\n",
    "                        else:\n",
    "                            print('####'.join(sent_lemmas),','.join(verb_spans),','.join(ent_spans),state_change(step_wise_properties[ent][prop][step_no+1],step_wise_properties[ent][prop][step_no]),sep='\\t',file=out_file)\n",
    "                    else:\n",
    "                        print(model)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_annotations()\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Parse all files for a given property. Pass empty property to take all props\n",
    "def parse_prop(filter_prop='isOpened'):\n",
    "    np.random.seed(1234)\n",
    "    print('\\n\\n',filter_prop)\n",
    "    train_f = open('./annotations/bert_annotations_'+filter_prop+'_train_comp.tsv','w',encoding='utf-8')\n",
    "    dev_f = open('./annotations/bert_annotations_'+filter_prop+'_dev_comp.tsv','w',encoding='utf-8')\n",
    "    test_f = open('./annotations/bert_annotations_'+filter_prop+'_test_comp.tsv','w',encoding='utf-8')\n",
    "    data_folder = './annotations/'\n",
    "    files = [train_f,dev_f,test_f]\n",
    "    count = 1\n",
    "    for folder in [fol for fol in listdir(data_folder) if not isfile(join(data_folder, fol))]:\n",
    "        folder = data_folder + folder+'/'\n",
    "        onlyfiles = [f[:-4] for f in listdir(folder) if f.endswith('.txt')]\n",
    "        for file in onlyfiles:\n",
    "            out_file = np.random.choice(files,p=[0.8,0.1,0.1])\n",
    "#             print(count, folder, file,out_file.name[-10:])\n",
    "            parse_annotations(out_file=out_file,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            count+=1\n",
    "    train_f.close()\n",
    "    dev_f.close()\n",
    "    test_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " isPowered\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isConnected\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isSetup\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isUsed\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isInstalled\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isRelatedDeviceConnected\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isSettingsChanged\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n",
      "\n",
      "\n",
      " isOpened\n",
      "'654_2___Set_up_an_FTP_Server_in_Ubuntu_Linux___Configuring_the_FTP_Server'\n",
      "'278_1___Change_Process_Priorities_in_Windows_Task_Manager___Steps'\n"
     ]
    }
   ],
   "source": [
    "######## PArse for all props\n",
    "for prop in property_event:\n",
    "    if(prop.startswith('is')):\n",
    "        parse_prop(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_annotations(open('temp.txt','w',encoding='utf-8'),169,'./annotations/Mac/','1017_1___Customize_Your_Mac_Using_the_Terminal___Using_Terminal_Commands','isOpened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_prop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### Parse the dataste for all folders for a given property\n",
    "def parse_prop_folders(filter_prop='isOpened'):\n",
    "    np.random.seed(1234)\n",
    "    print('\\n\\n',filter_prop)\n",
    "    train_f = open('./annotations/bert_annotations_'+filter_prop+'_train.tsv','w',encoding='utf-8')\n",
    "    dev_f = open('./annotations/bert_annotations_'+filter_prop+'_dev.tsv','w',encoding='utf-8')\n",
    "    \n",
    "    data_folder = './annotations/'\n",
    "    count = 1\n",
    "    for folder in [fol for fol in listdir(data_folder) if not isfile(join(data_folder, fol))]:\n",
    "        test_f = open('./annotations/bert_annotations_'+filter_prop+'_test_'+folder+'.tsv','w',encoding='utf-8')\n",
    "        files = [train_f,dev_f,test_f]\n",
    "        folder = data_folder + folder+'/'\n",
    "        onlyfiles = [f[:-4] for f in listdir(folder) if f.endswith('.txt')]\n",
    "        for file in onlyfiles:\n",
    "            out_file = np.random.choice(files,p=[0.8,0.1,0.1])\n",
    "            print(count, folder, file,out_file.name[-10:])\n",
    "            parse_annotations(out_file=out_file,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            count+=1\n",
    "        test_f.close()\n",
    "    train_f.close()\n",
    "    dev_f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_prop_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Prepare the dataset for ProLocal model\n",
    "def parse_prop_prolocal(filter_prop='isOpened'):\n",
    "    np.random.seed(1234)\n",
    "    print('\\n\\n',filter_prop)\n",
    "    train_f = open('./annotations/prolocal_annotations_'+filter_prop+'_train.tsv','w',encoding='utf-8')\n",
    "    dev_f = open('./annotations/prolocal_annotations_'+filter_prop+'_dev.tsv','w',encoding='utf-8')\n",
    "    test_f = open('./annotations/prolocal_annotations_'+filter_prop+'_test.tsv','w',encoding='utf-8')\n",
    "    data_folder = './annotations/'\n",
    "    files = [train_f,dev_f,test_f]\n",
    "    count = 1\n",
    "    for folder in [fol for fol in listdir(data_folder) if not isfile(join(data_folder, fol))]:\n",
    "        folder = data_folder + folder+'/'\n",
    "        onlyfiles = [f[:-4] for f in listdir(folder) if f.endswith('.txt')]\n",
    "        for file in onlyfiles:\n",
    "            out_file = np.random.choice(files,p=[0.8,0.1,0.1])\n",
    "            print(count, folder, file,out_file.name[-10:])\n",
    "            parse_annotations(out_file=out_file,doc_id=count, folder=folder, file=file, filter_prop=filter_prop,model='prolocal')\n",
    "            count+=1\n",
    "    train_f.close()\n",
    "    dev_f.close()\n",
    "    test_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_prop_prolocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Parse the dataset for common Bert model for all properties\n",
    "def parse_prop_common(filter_prop=''):\n",
    "    np.random.seed(1234)\n",
    "    print('\\n\\n',filter_prop)\n",
    "    train_f = open('./annotations/bert_annotations_'+filter_prop+'_train.tsv','w',encoding='utf-8')\n",
    "    dev_f = open('./annotations/bert_annotations_'+filter_prop+'_dev.tsv','w',encoding='utf-8')\n",
    "    test_f = open('./annotations/bert_annotations_'+filter_prop+'_test.tsv','w',encoding='utf-8')\n",
    "    test_files={}\n",
    "    for prop in property_event:\n",
    "        test_files[prop] = open('./annotations/bert_annotations_'+filter_prop+'_test_'+prop+'.tsv','w',encoding='utf-8')\n",
    "    data_folder = './annotations/'\n",
    "    files = [train_f,dev_f,test_f]\n",
    "    count = 1\n",
    "    for folder in [fol for fol in listdir(data_folder) if not isfile(join(data_folder, fol))]:\n",
    "        folder = data_folder + folder+'/'\n",
    "        onlyfiles = [f[:-4] for f in listdir(folder) if f.endswith('.txt')]\n",
    "        for file in onlyfiles:\n",
    "            out_file = np.random.choice(files,p=[0.8,0.1,0.1])\n",
    "            print(count, folder, file,out_file.name[-10:])\n",
    "            if(out_file==files[-1]):\n",
    "                parse_annotations(out_file=out_file,doc_id=count, folder=folder, file=file, filter_prop=filter_prop,test_files=test_files)\n",
    "            else:\n",
    "                parse_annotations(out_file=out_file,doc_id=count, folder=folder, file=file, filter_prop=filter_prop)\n",
    "            count+=1\n",
    "    train_f.close()\n",
    "    dev_f.close()\n",
    "    test_f.close()\n",
    "    for prop in property_event:\n",
    "        test_files[prop].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_prop_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:entities] *",
   "language": "python",
   "name": "conda-env-entities-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
